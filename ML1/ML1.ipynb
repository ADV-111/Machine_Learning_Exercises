{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# <center>Uczenie maszynowe</center>\n",
    "\n",
    "<br>\n",
    "\n",
    "<br>\n",
    "\n",
    "## <center>Proces rozwiązywania problemów z zastosowaniem uczenia maszynowego</center>\n",
    "\n",
    "<img src=\"Grafika/CRISP_DM_Process_Diagram.png\" width=\"350\">\n",
    "Źródło: https://upload.wikimedia.org/wikipedia/commons/thumb/b/b9/CRISP-DM_Process_Diagram.png/897px-CRISP-DM_Process_Diagram.png\n",
    "\n",
    "<br>\n",
    "\n",
    "<br>\n",
    "\n",
    "<br>\n",
    "\n",
    "<br>\n",
    "\n",
    "<br>\n",
    "\n",
    "\n",
    "\n",
    "\n",
    " <br>\n",
    "\n",
    "## Kilka faktów na temat uczenia maszynowego\n",
    "\n",
    "* ML to zarówno nauka jak i sztuka.\n",
    "\n",
    "* Nie istnieje metoda \"najlepsza\" - każdy problem wymaga indywidualnego podejścia.\n",
    "\n",
    "* Rozwiązanie problemu ML = reprezentacja danych + algorytm.\n",
    "\n",
    "  - w codziennej praktyce pierwszy czynnik często niedoceniany. Przetworzanie danych jest równie ważne jak same algorytmy, a nawet często dane są ważniejsze od algorytmu - często większy wpływ na wyniki ma postać danych (ich przygotowanie/przetworzenie) niż wybór konkretnego algorytmu.\n",
    "\n",
    "* Bardzo ważne jest zdefiniowanie celu jaki chcemy osiągnąć i rozumienie jak dane i algorytmy z tym celem się wiążą.\n",
    "\n",
    "\n",
    " <br>\n",
    " \n",
    " <br>\n",
    " \n",
    " <br>\n",
    " \n",
    " <br>\n",
    " \n",
    " <br>\n",
    " \n",
    " <br>\n",
    " \n",
    " <br>\n",
    " \n",
    " <br>\n",
    " \n",
    " ## Dwa główne nurty uczenia maszynowego:\n",
    "* uczenie nadzorowane (*supervised learning*)\n",
    "* uczenie nienadzorowane (*unsupervised learning*)\n",
    "\n",
    "  <br>\n",
    " \n",
    " <br>\n",
    " \n",
    " <br>\n",
    " \n",
    " <br>\n",
    " \n",
    " <br>\n",
    " \n",
    " <br>\n",
    " \n",
    " <br>\n",
    " \n",
    "## Uczenie nadzorowane\n",
    "\n",
    "Dla znanych par (X,Y) szukamy zależności między X a Y - budujemy model, który na podstawie X przewidzi Y.\n",
    " \n",
    "\n",
    "* regresja: Y jest zmienną rzeczywistą\n",
    "\n",
    "* klasyfikacja: Y jest zmienną dyskretną (np. binarną - o wartościach 0 i 1)\n",
    "\n",
    " <br>\n",
    " \n",
    " <br>\n",
    " \n",
    " <br>\n",
    " \n",
    " <br>\n",
    " \n",
    " <br>\n",
    " \n",
    " <br>\n",
    " \n",
    " <br>\n",
    " \n",
    "## Zagadnienie klasyfikacji - przykłady:\n",
    "- predykcja churnu - przewidywanie na podstawie cech klienta (opisujących m.in. jego zachowanie) czy klient odejdzie w najbliższym czasie (np. w ciągu miesiąca);\n",
    "- scoring kredytowy - przewidywanie czy klient ubiegający się o kredyt spłaci go w terminie;\n",
    "- rozpoznawanie choroby - rozpoznawanie na podstawie parametrów medycznych czy pacjent zachoruje/jest chory;\n",
    "- klasyfikacja tematyczna tekstu;\n",
    "- klasyfikacja wydźwięku opini;\n",
    "- rozpoznawanie zawartosci obrazów."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Regresja logistyczna\n",
    "\n",
    "Rozważamy problem klasyfikacji binarnej: $X \\in \\mathbb{R}^p$, $Y \\in \\{0, 1\\}$.\n",
    "\n",
    "Założenia modelu:\n",
    "\n",
    "$$\n",
    "Y =\n",
    "\\begin{cases}\n",
    "1, \\text{ z prawdopodobieństwem } \\pi(x)\\\\\n",
    "0, \\text{ z prawdopodobieństwem } 1-\\pi(x).\\\\\n",
    "\\end{cases}$$\n",
    "\n",
    ", gdzie\n",
    "\n",
    "$\\pi(x)=P(Y=1 \\mid x)$.\n",
    "\n",
    "Czyli innymi słowy:\n",
    "\n",
    "$$\\large Y \\sim B(\\pi(x)),$$\n",
    "\n",
    "\n",
    "B($\\cdot$) - rozkład dwupunktowy, \n",
    "\n",
    "\n",
    "Model:\n",
    "\n",
    "$$\\large \\pi(x) = \\frac{1}{1+e^{-\\beta x}},$$\n",
    "\n",
    "$\\beta = (\\beta_0,\\beta_1, \\ldots, \\beta_p),$   $\\beta x = \\beta_0 + \\beta_1 x_1 + \\ldots + \\beta_p x_p.$\n",
    "\n",
    " <br>\n",
    " \n",
    " <br>\n",
    " \n",
    "Predykcja wartości Y przy użyciu regresji logistycznej:\n",
    "\n",
    "$$\n",
    "\\hat{y} =\n",
    "\\begin{cases}\n",
    "1, \\text{ gdy } \\pi(x) > 0.5\\\\\n",
    "0, \\text{ gdy } \\pi(x) \\leq 0.5\\\\\n",
    "\\end{cases}\n",
    "$$\n",
    " \n",
    " <br>\n",
    " \n",
    " <br>\n",
    " \n",
    " <br>\n",
    "\n",
    "Dopasowanie modelu = znalezienie optymalnych wartości współczynników wektora $\\beta$, czyli takich, które najlepiej opisują zależność $Y$ od $X$. \n",
    "\n",
    " <br>\n",
    " \n",
    " <br>\n",
    " \n",
    " <br>\n",
    " \n",
    "Najlepiej czyli jak?\n",
    " \n",
    "## Metoda największej wiarogodności\n",
    " \n",
    "Rozważmy sytuację binarną. tzn. $y \\in \\{0,1\\}$. Logarytm funkcji wiarogodności wygląda następująco (X zbiór obserwacji, Y - zbiór etykiet):\n",
    "\n",
    "$ \n",
    "\\begin{align}\n",
    "L(\\beta \\ | \\ X,Y) & = \\log \\prod\\limits_{i=1}^n P(Y_i=y_i \\ | \\ x_i) \\\\\n",
    "       & = \\log \\prod\\limits_{i=1}^n \\pi(x_i)^{y_i}(1-\\pi(x_i))^{1-y_i} \\\\\n",
    "       & = \\sum\\limits_{i=1}^n \\log\\big( \\pi(x_i)^{y_i}(1-\\pi(x_i))^{1-y_i}\\big) \\\\\n",
    "       & = \\sum\\limits_{i=1}^n y_i\\log(\\pi(x_i)) + (1-y_i)\\log(1-\\pi(x_i)) .\n",
    "\\end{align}\n",
    "$\n",
    "\n",
    "Przyjmijmy oznaczenie: $h(\\pi(x),y) = y\\log{(\\pi(x))} + (1-y)\\log{(1-\\pi(x))}.$\n",
    "Zauważmy, że:\n",
    "\n",
    "$h(\\pi(x),1) = \\log{(\\pi(x))}$\n",
    "\n",
    "$h(\\pi(x),0) = \\log{(1-\\pi(x))}$\n",
    "\n",
    "Jeżeli $y=1$ to model jest tym lepszy im $\\pi(x)$ jest większe. Jeżeli $y=0$, to model jest tym lepszy im $\\pi(x)$ jest mniejsze, czyli $1 - \\pi(x)$ większe.\n",
    "\n",
    "Dopasowywanie modelu regresji logistycznej polega na maksymalizacji funkcji wiarogodności (technicznie - jej logarytmu) - szukamy (numerycznie) takiego wektora $\\beta$, dla którego wiarogodność jest największa.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Regularyzacja\n",
    "\n",
    "Regularyzacja - zabezpieczenie przed przeuczeniem.\n",
    "\n",
    "### Dopasowanie modelu\n",
    "\n",
    "<img src=\"Grafika/regularization.jpg\" width=\"600\">\n",
    "Źródło: https://i.ytimg.com/vi/nmHNXsDPPFQ/maxresdefault.jpg\n",
    "\n",
    "\n",
    "## Regularyzacja w regresji logistycznej\n",
    "\n",
    "Regularyzacja w regresji logistycznej polega na dodaniu do celu optymalizacyjnego kary za wielkości współczynników w $\\beta$.\n",
    "\n",
    "Przypomnijmy, że w regresji logistycznej estymator wektora $\\beta$ ma postać:\n",
    "\n",
    "$$\\hat{\\beta} = arg \\max\\limits_{\\beta} \\sum\\limits_{i=1}^n h(\\pi(x_i),y_i).$$\n",
    "\n",
    "W oczywisty sposób jest to równoważne rozwiązywaniu problemu:\n",
    "\n",
    "$$\\hat{\\beta} = arg \\min\\limits_{\\beta} -\\sum\\limits_{i=1}^n h(\\pi(x_i),y_i).$$\n",
    "\n",
    "### Regresja z regularyzacją L2:\n",
    "\n",
    "$$\\hat{\\beta} = arg \\min\\limits_{\\beta} \\big( -\\sum\\limits_{i=1}^n h(\\pi(x_i),y_i) + \\lambda\\|\\beta\\|_2^2 \\big),$$\n",
    "\n",
    "$\\|\\beta\\|_2$ - norma l2 wektora $\\beta$: $\\sqrt{\\sum\\limits_{i=1}^p\\beta_i^2}$,\n",
    "\n",
    "$\\lambda$ - współczynnik regularyzacji.\n",
    "\n",
    "Skąd wziąć wartość $\\lambda$? Trzeba wyznaczyć sobie empirycznie - przetestować model  z różnymi wartościami i wybrać tę, dla której wyniki są najlepsze.\n",
    "\n",
    "\n",
    "\n",
    "### Równie często stosowana jest norma l1: $\\sum\\limits_{i=1}^p|\\beta_i|$. \n",
    "\n",
    "#### Uwaga: Norma L1 zeruje współczynniki! \n",
    "\n",
    "\n",
    "Na marginesie: w modelach w uczeniu maszynowym pojawiają sie bardzo różne formy regularyzacji, których często nawet nie będziemy świadomi."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Jak stwierdzić czy model jest dobrze dopasowany? Czy nie za słabo, ani za mocno?\n",
    "\n",
    "1. Gdy na zbiorze testowym niski procent poprawnych predykcji (np. 60%), a na treningowym model bardzo dobrze dopasowany (np. 90%) -> model przeuczony\n",
    "\n",
    "2. Gdy na zbiorze treningowym model bardzo nisko dopasowany (np. rzędu pięćdziesiąt kilka procent) -> model słabo dopasowany.\n",
    "\n",
    "Co to jest niskie dopasowanie? -> zależy od danych...\n",
    "\n",
    "Co to jest duża różnica? -> zależy od danych... (i ich wielkości - istotność statystyczna!). Realnie, wyniki na testowym powinny być trochę niższe od dopasowania na treningowym (lub podobne). Klasyfikator w oczywisty sposób nie może działać lepiej niż dopasowanie na zbiorze treningowym - bo dopasowanie pokazuje jak dużo zależności wykrył w danych, na podstawie których będzie klasyfikował. Zatem jeżeli mamy dopasowanie na zbiorze treningowym np. 90%, to maksymalna moc predykcyjna jakiej możemy oczekiwać to 90%."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Przygotowanie danych - bardzo ważna sprawa!!!\n",
    "\n",
    "Czy wartości zmiennych mają wpływ na wynik modelu?\n",
    "\n",
    "Wyobraźmy sobie dwie zmienne, gdzie jedna ma wielkości rzędu 1, a druga rzędu 100, a rzeczywisty współczynnik przy pierwszej wynosi 1, a przy drugiej 0.01.\n",
    "\n",
    "Jak konsekwencje będzie miało zastosowanie regularyzacji?\n",
    "\n",
    "Zmienna pierwsza zostanie dużo mocniej \"ukarana\" niż druga, tzn. model mocno \"ściągnie\" współczynnik przy pierwszej, a zostawi przy drugiej...\n",
    "\n",
    "Jak temu zaradzić? -> skalowanie, standaryzacja"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Pipeline\n",
    "\n",
    "Gdy budowanie modelu składa się w rzeczywistości z dwóch kroków lub więcej - np. transformacja zmiennych, a potem dopasowanie modelu, to przydają się pipeline'y."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Optymalizacja parametrów\n",
    "\n",
    "GridSearch - próbowanie wszystkich możliwych kombinacji parametrów dla modelu"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Drzewo decyzyjne\n",
    "\n",
    "Załóżmy, że mamy dwie zmienne ($X[0], X[1]$) i problem klasyfikacji binarnej ($Y \\in \\{0,1\\}$).\n",
    "\n",
    "![Drzewo decyzyjne](Grafika/decision_tree.png)\n",
    "\n",
    "## Jak rośnie drzewno?\n",
    "\n",
    "Drzewo rośnie od korzenia - tzn. budujemy model od góry. Będąc w aktualnym węźle szukamy najlepszego podziału - rozpatrujemy wszystkie zmienne ze wszystkimi możliwymi podziałami i tworzymy rozgałęzienie według najlepszej opcji.\n",
    "\n",
    "Co to znaczy najlpszy podział? Rozważmy klasyfikację binarną.\n",
    "\n",
    "<br>\n",
    "\n",
    "<br>\n",
    "\n",
    "Opcja nr 1: Gini impurity (Miara Gini'ego niespójności węzła):\n",
    "- wybieramy podział, który minimalizuje ważoną miarę Giniego $g(p) = 2p(1-p)$:\n",
    "\n",
    "$$\\frac{n_1}{N}g(p_1) + \\frac{n_2}{N}g(p_2),$$\n",
    "\n",
    "gdzie $n_1, n_2$ liczności w pierwszym i drugim dziecku, $N = n_1 + n_2$, $p_1, p_2$ - procent obserwacji, dla których $Y=1$ w dzieciach 1 i 2.\n",
    "\n",
    "![Drzewo decyzyjne](Grafika/gini_entropy.png)\n",
    "\n",
    "<br>\n",
    "\n",
    "Opcja nr 2: Entropia.\n",
    "- wybieramy podział, który daje najmniejszą wartość ważonej entropii $h(p) = -p\\log{(p)} - (1-p)\\log{(1-p)}$:\n",
    "\n",
    "$$\\frac{n_1}{N} h(p_1) + \\frac{n_2}{N}h(p_2),$$\n",
    "\n",
    "gdzie $p_1, p_2$ - procent obserwacji, dla których $Y=1$ w dzieciach 1 i 2.\n",
    "\n",
    "## Do kiedy tworzymy nowe podziały? \n",
    "\n",
    "Możliwe są różne warunki stopu. Najpopularniejsze to:\n",
    "\n",
    "- maksymalna głębokość drzewa,\n",
    "- minimalna liczba obserwacji w liściu.\n",
    "\n",
    "Często stosowane w połączeniu."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Rysowanie drzewa decyzyjnego"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Python"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sklearn\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "model = DecisionTreeClassifier()\n",
    "model.fit(X,y)\n",
    "sklearn.tree.export_graphviz(model,out_file='tree.dot') "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Terminal"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Instalacja"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "```\n",
    "sudo apt install graphviz\n",
    "sudo apt-get -f install\n",
    "sudo apt install graphviz\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Wygenerowanie rysunku"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "``` \n",
    "dot -Tpng tree.dot -o tree.png\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "#### Zadanie \n",
    "\n",
    "Wczytaj zbiór `zbiór1.txt` (funkcja loadtxt w numpy) i podziel go na część uczącą (200 obserwacji) i testową (100). Dopasuj model regresji logistycznej na zbiorze uczącym i oblicz accuracy na zbiorze testowym. Następnie oblicz procent poprawnych dopasowań na zbiorze treningowym.\n",
    "\n",
    "Następnie narysuj 4 histogramy wartości obu zmiennych w pozbiorach wyznaczonych przez wartość $Y$. Czy widzisz jakąś zależność między zmiennymi objaśniającymi a zmienną objaśnianą?\n",
    "\n",
    "Następnie zbuduje drzewo klasyfikacyjne i policz dla niego accuracy."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Bardzo ważna sprawa: wnioskowanie o zależności zmiennych na podstawie analizy jednowymiarowej nie nie ma większego sensu!\n",
    "\n",
    "- jeśli nie widać zależności, to nie znaczy, że jej nie ma,\n",
    "- jeżeli widać zależność, to wcale nie musi oznaczać zależności przyczynowo-skutkowej (dana cecha może być skorelowana z inną cechą, która faktycznie wpływa na y).\n",
    "- zjawisko fałszywej korelacji - statystycznie rzecz biorąc, regularnie zdarzaja się sytuacje, kiedy zmienne wyglądają na zależne, a tak naprawde nie są...\n",
    "\n",
    "#### Naturalnie analiza wielowymiarowa (model) też może nie wykryć zależności... Sztuka analizy danych polega między innymi na szukaniu tych zależności."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Klasyfikacja wieloklasowa\n",
    "\n",
    "## Regresja logistyczna\n",
    "\n",
    "*(dokładniejszy opis: https://en.wikipedia.org/wiki/Multinomial_logistic_regression)*\n",
    "\n",
    "\n",
    "\\begin{align}\n",
    "\\Pr(Y_i=1) &= \\frac{1}{1+Z} e^{\\boldsymbol\\beta_1 \\cdot \\mathbf{X}_i} \\, \\\\\n",
    "\\Pr(Y_i=2) &= \\frac{1}{1+Z} e^{\\boldsymbol\\beta_2 \\cdot \\mathbf{X}_i} \\, \\\\\n",
    "\\cdots & \\cdots \\\\\n",
    "\\Pr(Y_i=K-1) &= \\frac{1}{1+Z} e^{\\boldsymbol\\beta_{K-1} \\cdot \\mathbf{X}_i} \\, \\\\\n",
    "\\Pr(Y_i=K) &= \\frac{1}{1+Z} \\, \\\\\n",
    "\\end{align}\n",
    "\n",
    "\n",
    "$Z = \\sum_{k=1}^{K-1} e^{\\boldsymbol\\beta_k \\cdot \\mathbf{X}_i}$\n",
    "\n",
    "### Definicja alternatywna\n",
    ":\n",
    "\n",
    "$$[P(Y=1), P(Y=2), \\ldots , P(Y=K)] = softmax([\\beta_1x_1, \\beta_2x_2, \\ldots , \\beta_Kx_K]) = \n",
    "[\\frac{e^{\\beta_1x_1}}{\\sum_{k=1}^{K}e^{\\beta_kx_k}}, \\frac{e^{\\beta_2x_2}}{\\sum_{k=1}^{K}e^{\\beta_kx_k}}, \\ldots, \\frac{e^{\\beta_Kx_K}}{\\sum_{k=1}^{K}e^{\\beta_k x_k}}]$$\n",
    "\n",
    "Na marginesie: w uczeniu maszynowym często pojawia sie funkcja \"softmax\".\n",
    "\n",
    "$$softmax(\\mathbf{x}) = [\\frac{e^{x_1}}{\\sum_{k=1}^{p}e^{x_k}}, \\frac{e^{x_2}}{\\sum_{k=1}^{p}e^{x_k}}, \\ldots, \\frac{e^{x_p}}{\\sum_{k=1}^{p}e^{x_k}}]$$\n",
    "\n",
    "\n",
    "\n",
    "## Drzewo decyzyjne\n",
    "\n",
    "Przyjmujemy oznaczenie: $p = (p_1, p_2, \\ldots, p_K)$ wektor prawdopodobieństw poszczególnych klas (procent obserwacji danej klasy).\n",
    "\n",
    "Opcja nr 1: Gini impurity (Miara Gini'ego niespójności węzła):\n",
    "- wybieramy podział, który minimalizuje ważoną miarę Gini'ego $g(p) = \\big( 1 - \\sum\\limits_{k=1}^K p_k^2 \\big)$:\n",
    "\n",
    "$$\\frac{n_1}{N} g(p_1) + \\frac{n_2}{N} g(p_2),$$\n",
    "\n",
    "gdzie $n_1, n_2$ liczności w pierwszym i drugim dziecku, $p_1, p_2$ - rozkłady klas w dzieciach.\n",
    "\n",
    "<br>\n",
    "\n",
    "Opcja nr 2: Entropia.\n",
    "- wybieramy podział, który daje najmniejszą wartość ważonej entropii $h(p) = -\\sum\\limits_{k=1}^K p_k\\log p_k$:\n",
    "\n",
    "$$\\frac{n_1}{n}\\sum\\limits_{i=1}^{n_1} h(p_1) + \\frac{n_2}{n}\\sum\\limits_{i=1}^{n_2} h(p_2)$$\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Zadanie\n",
    "\n",
    "Wygenerować predykcje kroswalidacyjnie dla regresji logistycznej i drzewa decyzyjnego (użyj `cross_val_predict`), a następnie wypisz accuracy i przedstaw tablicę klasyfikacji (`confusion_matrix` z `sklearn.metrics`) dla obu modeli."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn import datasets\n",
    "iris = datasets.load_iris()\n",
    "X = iris.data\n",
    "y = iris.target"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Naiwny klasyfikator Bayes'a\n",
    "\n",
    "Rozważmy klasyfikację wieloklasową - $Y \\in \\{1, 2, \\ldots, K\\}$. \n",
    "\n",
    "Naiwny klasyfikator Bayesa klasyfikuje obserwacje na podstawie prawdopodobieństwa:\n",
    "\n",
    "$$p(Y = k \\mid x_1, \\dots, x_p)  = p(C_k \\mid x_1, \\dots, x_p).$$\n",
    "\n",
    "Predykcja klasyfikatora to najbardziej prawdopodobna klasa.\n",
    "\n",
    "Skąd klasyfikator bierze to prawdopodobieństwo? Oblicza je przyjmując pewne założenia...\n",
    "\n",
    "Z twierdzenia Bayesa:\n",
    "\n",
    "\n",
    "$$p(C_k \\mid \\mathbf{x}) = \\frac{p(C_k) \\ p(\\mathbf{x} \\mid C_k)}{p(\\mathbf{x})}.$$\n",
    "\n",
    "Zauważmy, że $p(\\mathbf{x})$, czyli rozkład X, jest nieistotny dla klasyfikatora, zatem: \n",
    "\n",
    "$$p(C_k \\mid \\mathbf{x}) \\sim p(C_k) \\ p(\\mathbf{x} \\mid C_k).$$\n",
    "\n",
    "$p(C_k)$ - prawdopodobieśtwo a priori klasy $C_k$ - czyli procent obserwacji w danych, dla których $Y = k$.\n",
    "\n",
    "$p(\\mathbf{x} \\mid C_k) = p(x_1, x_2, \\ldots, x_p \\mid C_k)$ - rozkład cech w pozbiorze danych, dla których $Y = C_k$.\n",
    "\n",
    "\n",
    "(**Naiwne**) założenie modelu:\n",
    "\n",
    "$$p(x_1, x_2, \\ldots, x_p \\mid C_k) = p(x_1 \\mid C_k)\\cdot p(x_2\\mid C_k) \\cdot \\ldots \\cdot p(x_p \\mid C_k)$$\n",
    "\n",
    "Zatem predykcja klasyfikatora ma postać:\n",
    "\n",
    "$$\\hat{y} = arg \\max\\limits_k  p(C_k \\mid \\mathbf{x}) = arg \\max\\limits_k p(C_k) \\prod\\limits_{i=1}^p p(x_i\\mid C_k)$$\n",
    "\n",
    "Skąd bierzemy $p(x_i\\mid C_k)$?\n",
    "\n",
    "Z postaci danych:\n",
    "- jeżeli zmienna $x_i$ jest binarna, to zakładamy, że rozkład jest dwupunktowy - prawdopodobieństwo sukcesu model wylicza empirycznie z danych,\n",
    "- jeżeli zmienna $x_i$ jest licznością, to zakładamy, że rozkład jest wielomianowy - prawdopodobieństwa poszczególnych wartości model wylicza empirycznie z danych,\n",
    "- jeżeli zmienna $x_i$ jest rzeczywista, to zakładamy, że rozkład jest normalny - parametry (średnią i wariancję) model estymuje z danych.\n",
    "\n",
    "Uwaga: ostatnie wersja w praktyce z reguły działa słabo."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.naive_bayes import MultinomialNB # wersja wielomianowa"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "?MultinomialNB"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Case study\n",
    "\n",
    "Dane\n",
    "\n",
    "http://www.ritchieng.com/machine-learning-multinomial-naive-bayes-vectorization/"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(5572, 2)"
      ]
     },
     "execution_count": 39,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import pandas as pd\n",
    "\n",
    "sms = pd.read_table('Dane/sms.tsv', header=None, names=['label', 'message'])\n",
    "\n",
    "sms.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "# Cel: zaimplementować w pełni zautomatyzowane znajdowanie najlepszego modelu\n",
    "\n",
    "Na samym początku wydzielić 1000 obserwacji na zbiór testowy, na którym na samym końcu przetestujemy najlepszy model. Podczas szukania najlepszego modelu nie dotykamy tego zbioru!\n",
    "\n",
    "Wejście:\n",
    "- lista modeli w postaci ogólnie, którą podaje się do funkcji Pipeline (czyili listy krotek dwuelementwych),\n",
    "- lista siatek parametrów, które chcemy zbadać, dla odpowiadająych modeli,\n",
    "\n",
    "Wyjście \n",
    "- na zbiorze testowym policzyć accuracy najlepszego modelu.\n",
    "\n",
    "**Ważne** Każdą analizę (szukanie najlepszego modelu) zawsze należy zacząć od wyznaczenia punktu odniesienia. Czym powinien być punkt odniesienia? Jest to <u>prosta</u> metoda predykcji lub wręcz trywialna. Opcja 1: prosty klasyfikator z parametrami domyślnymi. Opcja 2. Predykcja trywialna - bezmodelowa, np. predykcja stała klasą dominującą (przykładowo jeżeli w danych jest np. $70\\%$ oberwacji klasy 1, to predykcja stała równa 1 będzie miała accuracy $70\\%$).\n",
    "\n",
    "Zatem krok pierwszy - obliczenie baseline."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn.naive_bayes import MultinomialNB\n",
    "from sklearn.preprocessing import MaxAbsScaler\n",
    "\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "\n",
    "import pprint\n",
    "from sklearn.model_selection import GridSearchCV, train_test_split, cross_val_score\n",
    "\n",
    "from sklearn.metrics import accuracy_score\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.1"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {},
   "toc_section_display": true,
   "toc_window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
